{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "taken-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile\n",
    "import nibabel as nb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import norm\n",
    "import pickle\n",
    "from nilearn import plotting\n",
    "from scipy import ndimage\n",
    "from kernel import kernel_conv\n",
    "from compile_studies import compile_studies\n",
    "from template import prior, affine, shape, pad_shape\n",
    "from main_effect import *\n",
    "from mytime import tic, toc\n",
    "\n",
    "THRESH = 0.001\n",
    "\n",
    "cwd = os.getcwd()\n",
    "raw_folder = f'{cwd}/DataRaw/'\n",
    "pickle_folder = f'{cwd}/DataPickle/'\n",
    "\n",
    "'''filename = askopenfilename()\n",
    "df = pd.read_excel(filename, engine='openpyxl', header=None)'''\n",
    "\n",
    "meta_df = pd.read_excel(raw_folder + 'Metastocalculate.xlsx', engine='openpyxl', header=None)\n",
    "with open(pickle_folder + 'experiments.pickle', 'rb') as f:\n",
    "    exp_all, tasks = pickle.load(f)\n",
    "\n",
    "row_idx = 3\n",
    "\n",
    "exp_name1 = meta_df.iloc[row_idx, 1]\n",
    "exp_idxs1 = compile_studies(meta_df, row_idx, tasks)\n",
    "exp_df1 = exp_all.loc[exp_idxs1].reset_index(drop=True)\n",
    "s1 = list(range(exp_df1.shape[0]))\n",
    "\n",
    "exp_name2 = meta_df.iloc[row_idx + 1, 1]\n",
    "exp_idxs2 = compile_studies(meta_df, row_idx+1, tasks)\n",
    "exp_df2 = exp_all.loc[exp_idxs2].reset_index(drop=True)\n",
    "s2 = list(range(exp_df2.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pediatric-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "mask_folder = f\"{cwd}/MaskenEtc/\"\n",
    "try:\n",
    "    os.makedirs(f\"{cwd}/ALE/rDiffs/Images\")\n",
    "    os.makedirs(f\"{cwd}/ALE/rDiffs/Null\")\n",
    "    os.mkdirs(f\"{cwd}/ALE/PMaps\")    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "uc = 0.001\n",
    "s0 = list(range(exp_df1.shape[0]))\n",
    "# highest possible ale value if every study had a peak at the same location.\n",
    "mb = 1\n",
    "for i in s0:\n",
    "    mb = mb*(1-np.max(exp_df1.at[i, 'Kernels']))\n",
    "\n",
    "# define bins for histogram\n",
    "bin_edges = np.arange(0.00005,1-mb+0.001,0.0001)\n",
    "bin_centers = np.arange(0,1-mb+0.001,0.0001)\n",
    "\n",
    "sample_n = 2500\n",
    "target_n = 17\n",
    "\n",
    "sample_space = np.array(np.where(prior == True))\n",
    "peaks = np.array([exp_df1.XYZ[i].T[:,:3] for i in s0], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "important-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(s0, sample_space, sample_n, target_n):\n",
    "    samples = np.zeros((sample_n, target_n))\n",
    "    s_perm = s0.copy()\n",
    "    for i in range(sample_n):\n",
    "        np.random.shuffle(s_perm)\n",
    "        samples[i,:] = s_perm[:target_n]\n",
    "    samples = np.unique(samples, axis=0)\n",
    "    unique_samples = samples.shape[0]\n",
    "    while unique_samples < sample_n:\n",
    "        np.random.shuffle(s_perm)\n",
    "        samples = np.vstack((samples, s_perm[:target_n]))\n",
    "        samples = np.unique(samples, axis=0)\n",
    "        unique_samples = samples.shape[0]\n",
    "    return samples.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "periodic-fireplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:   35.1s\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed:   39.8s\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed:   49.7s\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:   56.4s\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed:   59.9s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "# Cluster Null\n",
    "max_ale, max_cluster, thresh = zip(*Parallel(n_jobs=8, verbose=10)(delayed(compute_noise_max)(s0 = s1,\n",
    "                                                                                              sample_space = sample_space,\n",
    "                                                                                              num_peaks = exp_df1.Peaks,\n",
    "                                                                                              kernels = exp_df1.Kernels,\n",
    "                                                                                              bin_centers=bin_centers,\n",
    "                                                                                              bin_edges=bin_edges,\n",
    "                                                                                              target_n=17) for i in range(500)))\n",
    "cut_cluster = np.percentile(max_cluster, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "precious-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = create_samples(s1, sample_space, sample_n, target_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "challenging-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubALE\n",
    "ma = compute_ma(s0, peaks, exp_df1.Kernels)\n",
    "hx = compute_hx(s0, ma, bin_edges)\n",
    "\n",
    "\n",
    "p_map = np.zeros((sample_n,sample_space[0].size))\n",
    "ale = compute_ale(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "lonely-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pmap(sample, ma, hx, bin_centers, cut_cluster):\n",
    "    ale_null = compute_ale_null(sample, hx, bin_centers)\n",
    "    ale = compute_ale(ma[sample])\n",
    "    z = compute_z(ale, ale_null)\n",
    "    z, max_cluster = compute_cluster(z, THRESH, sample_space=None, cut_cluster=cut_cluster)\n",
    "    z[z > 0] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "geographic-somerset",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:   34.5s\n",
      "[Parallel(n_jobs=8)]: Done 632 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "prob_act = Parallel(n_jobs=8, verbose=2)(delayed(compute_pmap)(samples[i], ma, hx, bin_centers, 87) for i in range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "controversial-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_act = np.mean(prob_act, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-seeking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
