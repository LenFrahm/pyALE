{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import nibabel as nb\n",
    "import math as m\n",
    "import pickle\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "from compile_studies import compile_studies\n",
    "from tfce import tfce\n",
    "from scipy.io import loadmat\n",
    "from simulate_noise import simulate_noise\n",
    "from scipy import ndimage\n",
    "from mytime import tic, toc\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "cwd = os.getcwd()\n",
    "raw_folder = cwd + '/DataRaw/'\n",
    "pickle_folder = cwd + '/DataPickle/'\n",
    "mask_folder = cwd + '/MaskenEtc/'\n",
    "\n",
    "'''filename = askopenfilename()\n",
    "df = pd.read_excel(filename, engine='openpyxl', header=None)'''\n",
    "\n",
    "df = pd.read_excel(raw_folder + 'Metastocalculate.xlsx', engine='openpyxl', header=None)\n",
    "with open(pickle_folder + 'experiments.pickle', 'rb') as f:\n",
    "    experiments, tasks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other : 19experiments; average of 22.263157894736842 subjects per experiment\n",
      "Cwclassic : 37experiments; average of 25.756756756756758 subjects per experiment\n",
      "Cwvariant : 21experiments; average of 26.333333333333332 subjects per experiment\n",
      "CwIvsC : 58experiments; average of 25.96551724137931 subjects per experiment\n",
      "CwIvsN : 41experiments; average of 22.21951219512195 subjects per experiment\n",
      "CwIvsNwords : 17experiments; average of 22.235294117647058 subjects per experiment\n",
      "CwIvsNsymbolsletters : 23experiments; average of 22.304347826086957 subjects per experiment\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if row[0] == 'M': #Main Effect Analysis\n",
    "        s0 = compile_studies(df, index, experiments, tasks)\n",
    "        if len(s0) >= 12:\n",
    "            category = df.iloc[index, 1]\n",
    "            mean_subjects = experiments.iloc[s0].Subjects.mean()\n",
    "            print(category + ' : ' + str(len(s0)) + 'experiments; average of {mean} subjects per experiment'.format(mean=mean_subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComputeALEtfce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other - loading Foci\n",
      "Other - loading ALE\n",
      "Other - loading null PDF\n",
      "Other - loading p-values\n",
      "Other - simulating noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:   39.5s\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 215 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=3)]: Done 359 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=3)]: Done 539 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=3)]: Done 642 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=3)]: Done 755 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=3)]: Done 876 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed: 10.4min finished\n"
     ]
    }
   ],
   "source": [
    "s0 = compile_studies(df, 0, experiments, tasks)\n",
    "experiments = experiments.loc[s0]\n",
    "study = 'Other'\n",
    "\n",
    "template = nb.load(mask_folder + \"Grey10.nii\")\n",
    "template_data = template.get_fdata()\n",
    "template_shape = template_data.shape\n",
    "pad_tmp_shape = [value+30 for value in template_shape]\n",
    "\n",
    "\n",
    "prior = np.zeros(template_shape, dtype=bool)\n",
    "prior[template_data > 0.1] = 1\n",
    "uc = 0.001\n",
    "c = str(uc)[2:]\n",
    "eps = np.finfo(float).eps\n",
    "to_repeat = 1000\n",
    "\n",
    "num_exp = len(s0)\n",
    "mb = 1\n",
    "for i in s0:\n",
    "    mb = mb*(1-np.max(experiments.at[i, 'Kernel']))\n",
    "\n",
    "bin_edge = np.arange(0.00005,1-mb+0.001,0.0001)\n",
    "bin_center = np.arange(0,1-mb+0.001,0.0001)\n",
    "step = 1/0.0001\n",
    "\n",
    "folders_req = ['Volumes', 'NullDistributions', 'VolumesZ', 'VolumesTFCE', 'Results', 'Images', 'Foci']\n",
    "folders_req_imgs = ['Foci', 'ALE', 'TFCE']\n",
    "\n",
    "try:\n",
    "    os.mkdir('ALE')\n",
    "    for folder in folders_req:\n",
    "        os.mkdir('ALE/' + folder)\n",
    "        if folder == 'Images':\n",
    "            for sub_folder in folders_req_imgs:\n",
    "                os.mkdir('ALE/Images/' + sub_folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "if isfile(cwd + '/ALE/Foci/' + study + '.nii'):\n",
    "    print('{} - loading Foci'.format(study))\n",
    "    foci_arr = nb.load(cwd + '/ALE/Foci/' + study + '.nii').get_fdata()\n",
    "else:\n",
    "    print('{} - illustrate Foci'.format(study))\n",
    "    \n",
    "    foci_arr = np.zeros(template_shape)\n",
    "    nested_list = [experiments.XYZ[i].T[:,:3].tolist() for i in s0]\n",
    "    flat_list = np.array([item for sublist in nested_list for item in sublist])\n",
    "    foci_arr[tuple(flat_list.T)] += 1\n",
    "    foci_arr[~prior] = np.nan \n",
    "    ni_img = nb.Nifti1Image(foci_arr, template.affine)\n",
    "    nb.save(ni_img, cwd + '/ALE/Foci/' + study + '.nii')\n",
    "    \n",
    "\n",
    "\n",
    "if isfile(cwd + '/ALE/NullDistributions/' + study + '.pickle'):\n",
    "    print('{} - loading ALE'.format(study))\n",
    "    print('{} - loading null PDF'.format(study))\n",
    "    ale_arr = nb.load(cwd + '/ALE/Volumes/' + study + '.nii').get_fdata()\n",
    "    ale_arr = np.nan_to_num(ale_arr)\n",
    "    with open(cwd + '/ALE/NullDistributions/' + study + '.pickle', 'rb') as f:\n",
    "            ale_hist, last_used, c_null = pickle.load(f)\n",
    "else:\n",
    "    print('{} - computing ALE'.format(study))\n",
    "    \n",
    "    ale_arr = np.ones(template_shape)\n",
    "    hx = np.zeros((len(s0),len(bin_edge)))\n",
    "    for c, i in enumerate(s0):\n",
    "        data = np.zeros(pad_tmp_shape)\n",
    "        for ii in range(experiments.at[i, 'Peaks']):\n",
    "            coords = experiments.XYZ[i].T[:,:3][ii]\n",
    "            x_range = (coords[0],coords[0]+31)\n",
    "            y_range = (coords[1],coords[1]+31)\n",
    "            z_range = (coords[2],coords[2]+31)\n",
    "            data[x_range[0]:x_range[1], y_range[0]:y_range[1], z_range[0]:z_range[1]] = \\\n",
    "            np.maximum(data[x_range[0]:x_range[1], y_range[0]:y_range[1], z_range[0]:z_range[1]],\n",
    "                       experiments.at[i, 'Kernel'])\n",
    "        data = data[15:data.shape[0]-15,15:data.shape[1]-15, 15:data.shape[2]-15]\n",
    "        bin_idxs, counts = np.unique(np.digitize(data[prior], bin_edge),return_counts=True)\n",
    "        hx[c,bin_idxs] = counts\n",
    "        ale_arr = np.multiply(ale_arr, 1-data)\n",
    "\n",
    "    ale_arr = 1-ale_arr\n",
    "    ale_arr[~prior] = np.nan\n",
    "    \n",
    "    #Save ALE scores in Nifti\n",
    "    ni_img = nb.Nifti1Image(ale_arr, template.affine)\n",
    "    nb.save(ni_img, cwd + '/ALE/Volumes/' + study + '.nii')\n",
    "\n",
    "    \n",
    "    print('{} - permutation-null PDF'.format(study))\n",
    "    step = 1/np.mean(np.diff(bin_center))\n",
    "    ale_hist = hx[0,:]\n",
    "    for i in range(1,len(s0)):\n",
    "        v1 = ale_hist\n",
    "        v2 = hx[i,:]\n",
    "\n",
    "        da1 = np.where(v1 > 0)[0]\n",
    "        da2 = np.where(v2 > 0)[0]\n",
    "\n",
    "        v1 = ale_hist/np.sum(v1)\n",
    "        v2 = hx[i,:]/np.sum(v2)\n",
    "\n",
    "        ale_hist = np.zeros((len(bin_center),))\n",
    "        for i in range(len(da2)):\n",
    "            p = v2[da2[i]]*v1[da1]\n",
    "            score = 1-(1-bin_center[da2[i]])*(1-bin_center[da1])\n",
    "            ale_bin = np.round(score*step).astype(int)\n",
    "            ale_hist[ale_bin] = np.add(ale_hist[ale_bin], p)\n",
    "\n",
    "    last_used = np.where(ale_hist>0)[0][-1]\n",
    "    c_null = np.flip(np.cumsum(np.flip(ale_hist[:last_used+1])))\n",
    "\n",
    "    pickle_object = (ale_hist, last_used, c_null)\n",
    "    with open(cwd + '/ALE/NullDistributions/' + study + '.pickle', \"wb\") as f:\n",
    "        pickle.dump(pickle_object, f)\n",
    "\n",
    "if isfile(cwd + '/ALE/VolumesTFCE/' + study + '.nii'):\n",
    "    print('{} - loading p-values'.format(study))\n",
    "    \n",
    "    z_arr = nb.load(cwd + '/ALE/VolumesZ/' + study + '.nii').get_fdata()\n",
    "    z_arr = np.nan_to_num(z_arr)\n",
    "    \n",
    "    tfce_arr = nb.load(cwd + '/ALE/VolumesTFCE/' + study + '.nii').get_fdata()\n",
    "else:\n",
    "    print('{} - computing p-values'.format(study))\n",
    "    \n",
    "    ale_step = np.round(ale_arr*step)\n",
    "    palette, index = np.unique(ale_step, return_inverse=True)\n",
    "    index = palette[index].astype(int)\n",
    "    p = c_null[index].reshape(ale_step.shape)\n",
    "    p[p < eps] = eps\n",
    "    z_arr = norm.ppf(1-p)\n",
    "    \n",
    "    tfce_arr = tfce(invol=z_arr, voxel_dims=template.header.get_zooms())\n",
    "    tfce_arr[~prior] = np.nan\n",
    "    \n",
    "    tfce_img = nb.Nifti1Image(tfce_arr, template.affine)\n",
    "    nb.save(tfce_img, cwd + '/ALE/VolumesTFCE/' + study + '.nii')\n",
    "    \n",
    "    z_arr[~prior] = np.nan\n",
    "    z_img = nb.Nifti1Image(z_arr, template.affine)\n",
    "    nb.save(z_img, cwd + '/ALE/VolumesZ/' + study + '.nii')\n",
    "\n",
    "if isfile(cwd+\"/ALE/NullDistributions/\" + study + \"_clustP.pickle\"):\n",
    "    print('{} - loading noise'.format(study))\n",
    "    with open(cwd+\"/ALE/NullDistributions/\" + study + \"_clustP.pickle\", 'rb') as f:\n",
    "        max_ale, max_cluster_size, max_tfce = pickle.load(f)\n",
    "else:\n",
    "    print('{} - simulating noise'.format(study))\n",
    "    num_peaks = experiments.loc[:,'Peaks']\n",
    "    kernels = experiments.loc[:,'Kernel']\n",
    "\n",
    "    permSpace5 = loadmat(\"MaskenEtc/permSpace5.mat\")\n",
    "    sample_space = permSpace5[\"allXYZ\"]\n",
    "    delta_t = np.max(z_arr)/100\n",
    "\n",
    "    nm, nn, nt = zip(*Parallel(n_jobs=3, verbose=8)(delayed(simulate_noise)(sample_space = sample_space,\n",
    "                                                                     s0 = s0,\n",
    "                                                                     num_peaks = num_peaks,\n",
    "                                                                     kernels = kernels,\n",
    "                                                                     c_null = c_null,\n",
    "                                                                     tfce_params = [delta_t, 0.6, 2]) for i in range(to_repeat)))\n",
    "\n",
    "    \n",
    "    simulation_pickle = (nm, nn, nt)\n",
    "    with open(cwd+\"/ALE/NullDistributions/\" + study + \"_clustP.pickle\", \"wb\") as f:\n",
    "        pickle.dump(simulation_pickle, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
