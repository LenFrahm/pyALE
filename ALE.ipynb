{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "united-integral",
   "metadata": {},
   "source": [
    "# ALE\n",
    "\n",
    "This notebook will guide you through the process of setting up and running an ALE meta-analysis.\n",
    "\n",
    "First we need to set up two excel files that will serve as input for all following steps:\n",
    "\n",
    "#### 1. Experiment information\n",
    "\n",
    "Please use the following format / syntax:\n",
    "\n",
    "- The spreadsheet should have a title row that will not be read\n",
    "- The order of the columns should be as in this example\n",
    "- Tags are optional\n",
    "\n",
    "![title](img/exp_info.png)\n",
    "\n",
    "**Important note: In either case the tags are taken straight as they are, so please make sure that spelling (including spaces etc) is consistent throughout the file!**\n",
    "\n",
    "\n",
    "#### 2. Analysis information\n",
    "\n",
    "Please use the following format / syntax:\n",
    "\n",
    "![title](img/meta_info.png)\n",
    "\n",
    "**1st column**: \n",
    "- M denotes main effect\n",
    "- P denotes probabilistic / subsampled / CV'd ALE\n",
    "    - Need to specify the desired N after the P, e.g. P17\n",
    "- C denotes contrast (this line and the following are contrasted against each other). If the respective main effects in the two lines [e.g. 3 and 4] are not yet computed, they will be evaluated\n",
    "- B denotes balanced contrast. This way both analysis will be based on the same N. (Option to specify the desired N after the B, e.g. B17)\n",
    "    - Option to specify the desired N after the B, e.g. B17. If no N is provided N will be equal to the smaller analysis N - 2 or the mean between the smaller analysis N and 17 depending on which is smaller.\n",
    "\n",
    "**2nd column**:\n",
    "Titel of the effect. Ideally avoid spaces and non file-name characters. When (re-) entering a title as part of a contrast, please keep naming/spelling consistent otherwise the “old” main effect is not recognized\n",
    "\n",
    "**3rd column onwards: Specify the analysis by tags.**\n",
    "- +ALL   ->   Include the entire set of experiments in that matfile\n",
    "- +Tag   ->    Include only those experiments that have the respective label\n",
    "- +Tag1  +Tag2   -> Conjunction, includes only experiments that have both labels\n",
    "- +Tag1  +Tag2   ?   ->  Logical ‘OR’, includes experiments that have either label\n",
    "- -Tag   ->   Exclude experiments that have the respective label\n",
    "- $VOI   ->  additionally to the voxel-wise analysis perform VOI analysis for the specified VOI [full path/filename, binary mask]. Multiple VOIs can be entered after each other and are evaluated at the same time. Furthermore an atlas can be added and a ROI analysis will be run for each distinct region specified in the atlas\n",
    "\n",
    "## How to run your analyses\n",
    "\n",
    "1. Create a folder in which you would like to run your analysis (can be the folder you cloned this repository)\n",
    "2. Copy both excel sheets into the folder\n",
    "3. Enter the path to your folder and the names of your two excel sheets into the code cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "photographic-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"/Users/lfrahm/Code/ALE/pyALE/\"\n",
    "analysis_info_name = \"analysis_info.xlsx\"\n",
    "experiment_info_name = \"experiment_info.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-sacrifice",
   "metadata": {},
   "source": [
    "4. Run your analysis by running the code cells below. There will be output printed below indicating your process.\n",
    " \n",
    "**Attention: Depending on your knowledge level you can either run the basic routine taking over all default parameter values or run the advanced routine, where you can change the parameter settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "built-budget",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lfrahm/.pyenv/versions/3.9.1/envs/pyALE/lib/python3.9/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "from nb_pipeline import setup, analysis\n",
    "meta_df, exp_all, tasks = setup(wd, analysis_info_name, experiment_info_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "normal-calculation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other : 19 experiments; average of 22.26 subjects per experiment\n",
      "Other - illustrate Foci\n",
      "Other - computing ALE and null PDF\n",
      "Other - computing p-values & TFCE\n",
      "Other - simulating null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c55de6c98ad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"Basic routine\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0manalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Code/ALE/pyALE/nb_pipeline.py\u001b[0m in \u001b[0;36manalysis\u001b[0;34m(path, meta_df, exp_all, tasks, null_repeats, cluster_thresh, sample_n, diff_thresh, diff_repeats)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_idxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{exp_name} : {len(exp_idxs)} experiments; average of {exp_df.Subjects.mean():.2f} subjects per experiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mmain_effect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnull_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mcontribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/ALE/pyALE/main_effect.py\u001b[0m in \u001b[0;36mmain_effect\u001b[0;34m(exp_df, exp_name, bin_steps, cluster_thresh, null_repeats, target_n, sample_n)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# peaks are randomly distributed in the sample space. Then calculate all metrics that have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# been calculated for the 'actual' data to create a null distribution unde the assumption of indipendence of results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             null_ale, max_ale, max_cluster, max_tfce = zip(*Parallel(n_jobs=-1, verbose=1)\n\u001b[0m\u001b[1;32m    131\u001b[0m                                                           (delayed(compute_null_cutoffs)(s0=s0,\n\u001b[1;32m    132\u001b[0m                                                                                          \u001b[0msample_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.1/envs/pyALE/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.1/envs/pyALE/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.1/envs/pyALE/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Basic routine\"\"\"\n",
    "analysis(wd, meta_df, exp_all, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "powered-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Advanced Routine\"\"\"\n",
    "\n",
    "# amount of reptitions used for null distribution simulation (noise samples). Convergence starts at around 1000, recommended value = 10000\n",
    "# Used in all types of analysis\n",
    "null_repeats=1000\n",
    "\n",
    "# significance threshold used for cluster forming. recommended value = 0.001\n",
    "# Used in full MainEffect analysis\n",
    "cluster_thresh=0.001\n",
    "\n",
    "# Amount of samples taken from study population, recommended value = 2500\n",
    "# Used in probabilistic and balanced contrast\n",
    "sample_n=2500\n",
    "\n",
    "# Threshold used to compare differences against. recommended value = 0.05\n",
    "# Used in (\"legacy\") contrast\n",
    "diff_thresh=0.05\n",
    "\n",
    "# Amount of sampling repititions used when comparing two subsampled ALEs. recommended value > 500\n",
    "# Used in balanced contrast\n",
    "# Note: As this difference permutation is performed inside of the null distribution simulation it gets repeated X (null_repeats) amount of times\n",
    "# - this can lead to a small increase in diff_repeats leading to a big increase in computation time\n",
    "diff_repeats=500\n",
    "\n",
    "analysis(wd, meta_df, exp_all, tasks, null_repeats, cluster_thresh, sample_n, diff_thresh, diff_repeats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
